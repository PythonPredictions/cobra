{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cobra requires the usual Python packages for data science:\n",
    "- numpy (>=1.19.4)\n",
    "- pandas (>=1.1.5)\n",
    "- scipy (>=1.5.4)\n",
    "- scikit-learn (>=0.23.1)\n",
    "- matplotlib (>=3.3.3)\n",
    "- seaborn (>=0.11.0)\n",
    "\n",
    "These packages, along with their versions are listed in requirements.txt and can be installed using pip.\n",
    "\n",
    "\n",
    "Note: if you want to install cobra with e.g. pip, you don't have to install all of these requirements as these are automatically installed with cobra itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to install cobra is using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pythonpredictions-cobra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section we will walk you through all the required steps to build a predictive logistic regression model using **Cobra**. All classes and functions used here are well-documented. In case you want more information on a class or function, run the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(function_or_class_you_want_info_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a good model involves three steps\n",
    "\n",
    "1. **Preprocessing**: properly prepare the predictors (a synonym for “feature” or variable that we use throughout this tutorial) for modelling.\n",
    "\n",
    "2. **Feature Selection**: automatically select a subset of predictors which contribute most to the target variable or output in which you are interested.\n",
    "\n",
    "3. **Model Evaluation**: once a model has been build, a detailed evaluation can be performed by computing all sorts of evaluation metrics.\n",
    "\n",
    "\n",
    "\n",
    "Let's dive in!!!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Prediction using Titanic data\n",
    "- GOAL : Predict if individuals survives in titanic sinking\n",
    "- BASETABLE : seaborn dataset Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "from cobra.preprocessing import PreProcessor\n",
    "from cobra.evaluation import generate_pig_tables, plot_incidence\n",
    "from cobra.evaluation import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df=sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we assume the data for model building is available in a pandas DataFrame. This DataFrame should contain a an ID column, a target column (e.g. “**survived**”) and a number of candidate predictors (features) to build a model with.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is required to set all category vars to object dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.dtypes == 'category'] =\\\n",
    "    df.select_dtypes(['category'])\\\n",
    "    .apply(lambda x: x.astype('object'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first part focusses on preparing the predictors for modelling by:\n",
    "\n",
    "1. Defining the ID column, the target, discrete and contineous variables\n",
    "\n",
    "2. Splitting the dataset into training, selection and validation datasets.\n",
    "\n",
    "3. Binning continuous variables into discrete intervals\n",
    "\n",
    "4. Replacing missing values of both categorical and continuous variables (which are now binned) with an additional “Missing” bin/category\n",
    "\n",
    "5. Regrouping categories in new category “other”\n",
    "\n",
    "6. Replacing bins/categories with their corresponding incidence rate per category/bin.\n",
    "\n",
    "*Disclaimer*: Cobra's Preprocesser is valid only if the original data does not contain extreme irregularities, such as outliers or very skewed distributions. This should always be checked beforehand by its user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy dataset, the index will serve as ID,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df.index + 1\n",
    "id_col = \"id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and survived is the target,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"survived\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove the columns 'who' and 'adult_male' since they are duplicate of 'sex', and also 'alive', which seems to be a duplicate of 'survived'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['who']\n",
    "del df['adult_male']\n",
    "del df['alive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out which variables are categorical (\"discrete\") and which are continous:\n",
    "\n",
    "\n",
    " => discrete are definitely those that contain strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dtypes = df.dtypes\n",
    "discrete_vars = [col for col in col_dtypes[col_dtypes==object].index.tolist() if col not in [id_col, target_col]] \n",
    "print(discrete_vars)\n",
    "print()\n",
    "for col in discrete_vars:\n",
    "    print(col)\n",
    "    print(df[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also check for numerical columns that only contain a few different values, thus to be interpreted as discrete, categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col not in discrete_vars and col not in [id_col, target_col]: # if we didn't mark it as discrete already because it was string typed, or also excluding it if it is the target:\n",
    "        val_counts = df[col].value_counts()\n",
    "        if len(val_counts) > 1 and len(val_counts) <= 10: # The column contains less than 10 different values. \n",
    "            print(col)\n",
    "            print(val_counts)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the printed variables, it is clear that we have to include those in the list of discrete variables. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_vars.extend([\"pclass\",\"sibsp\",\"parch\",\"class\",\"deck\",\"alone\"])\n",
    "discrete_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining variables can be labelled continous predictors, without including the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_vars = list(set(df.columns)\n",
    "                       - set(discrete_vars) \n",
    "                       - set([id_col, target_col]))\n",
    "continuous_vars                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can prepare **Cobra's Preprocessor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all Cobra's default parameters for preprocessing for now:\n",
    "preprocessor = PreProcessor.from_params(\n",
    "    model_type=\"classification\")\n",
    "\n",
    "# These are the options though:\n",
    "help(PreProcessor.from_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into train-selection-validation set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.preprocessing import PreProcessor\n",
    "basetable = preprocessor.train_selection_validation_split(\n",
    "                data=df,\n",
    "                train_prop=0.6,\n",
    "                selection_prop=0.2,\n",
    "                validation_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit the preprocessor pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(basetable[basetable[\"split\"] == \"train\"],\n",
    "                 continuous_vars=continuous_vars,\n",
    "                 discrete_vars=discrete_vars,\n",
    "                 target_column_name=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline can now be performed on the basetable!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable = preprocessor.transform(basetable,\n",
    "                                   continuous_vars=continuous_vars,\n",
    "                                   discrete_vars=discrete_vars)\n",
    "basetable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the predictors are properly prepared, we can start building a predictive model, which boils down to selecting the right predictors from the dataset to train a model on.\n",
    "As a dataset typically contains many predictors, **we first perform a univariate preselection** to rule out any predictor with little to no predictive power. Later, using the list of preselected features, we build a logistic regression model using **forward feature selection** to choose the right set of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous steps, these were the predictors, as preprocessed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_predictors = [\n",
    "    col for col in basetable.columns\n",
    "    if col.endswith(\"_bin\") or col.endswith(\"_processed\")]\n",
    "sorted(preprocessed_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for feature selection, we use the target encoded version of each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_predictors = [col for col in basetable.columns.tolist()\n",
    "                           if '_enc' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A univariate selection on the preprocessed predictors can be conducted. The thresholds for retaining a feature are now on default but can be changed by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.model_building import univariate_selection\n",
    "\n",
    "df_auc = univariate_selection.compute_univariate_preselection(\n",
    "    target_enc_train_data=basetable[basetable[\"split\"] == \"train\"],\n",
    "    target_enc_selection_data=basetable[basetable[\"split\"] == \"selection\"],\n",
    "    predictors=preprocessed_predictors,\n",
    "    target_column=target_col,\n",
    "    preselect_auc_threshold=0.53,  # if auc_selection <= 0.53 exclude predictor\n",
    "    preselect_overtrain_threshold=0.05  # if (auc_train - auc_selection) >= 0.05 --> overfitting!\n",
    "    )\n",
    "from cobra.evaluation import plot_univariate_predictor_quality\n",
    "plot_univariate_predictor_quality(df_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute correlations between the preprocessed predictors and plot it using a correlation matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.evaluation import plot_correlation_matrix\n",
    "df_corr = (univariate_selection\n",
    "           .compute_correlations(basetable[basetable[\"split\"] == \"train\"],\n",
    "                                 preprocessed_predictors))\n",
    "plot_correlation_matrix(df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a list of the selected predictors after the univariate selection, run the following call:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselected_predictors = (univariate_selection\n",
    "                          .get_preselected_predictors(df_auc))\n",
    "preselected_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an initial preselection on the predictors, we can start building the model itself using forward feature selection to choose the right set of predictors. Since we use target encoding on all our predictors, we will only consider models with positive coefficients (no sign flip should occur) as this makes the model more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.model_building import ForwardFeatureSelection\n",
    "\n",
    "forward_selection = ForwardFeatureSelection(model_type=\"classification\",\n",
    "                                            max_predictors=30,\n",
    "                                            pos_only=True)\n",
    "\n",
    "# fit the forward feature selection on the train data\n",
    "# has optional parameters to force and/or exclude certain predictors (see docs)\n",
    "forward_selection.fit(basetable[basetable[\"split\"] == \"train\"],\n",
    "                      target_column_name = target_col,\n",
    "                      predictors = preselected_predictors)\n",
    "                      #forced_predictors: list = [],\n",
    "                      #excluded_predictors: list = [])\n",
    "\n",
    "# compute model performance\n",
    "performances = (forward_selection\n",
    "                .compute_model_performances(basetable, target_column_name = target_col))\n",
    "performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, we have completed 4 steps till no further improvement can be observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.evaluation import plot_performance_curves\n",
    "\n",
    "# plot performance curves\n",
    "plot_performance_curves(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performance curves (AUC per model with a particular number of predictors in case of logistic regression), a final model can then be chosen and the variables importance can be plotted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = forward_selection.get_model_from_step(3)\n",
    "\n",
    "# Note that chosen model the following variables:\n",
    "final_predictors = model.predictors\n",
    "print(final_predictors)\n",
    "from cobra.evaluation import plot_variable_importance\n",
    "\n",
    "variable_importance = model.compute_variable_importance(\n",
    "    basetable[basetable[\"split\"] == \"selection\"]\n",
    ")\n",
    "plot_variable_importance(variable_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: variable importance is based on correlation of the predictor with the model scores (and not the true labels!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Finally, we can again export the model to a dictionary to store it as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model.serialize()\n",
    "\n",
    "model_path = os.path.join(\"output\", \"model.json\")\n",
    "with open(model_path, \"w\") as file:\n",
    "    json.dump(model_dict, file)\n",
    "\n",
    "# To reload the model again from a JSON file, run the following snippet:\n",
    "# from cobra.model_building import LinearRegressionModel\n",
    "# with open(model_path, \"r\") as file:\n",
    "#     model_dict = json.load(file)\n",
    "# model = LinearRegressionModel()\n",
    "# model.deserialize(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have build and selected a final model, it is time to evaluate its predictions on the test set against various evaluation metrics. The used evaluation metrics are:\n",
    "1. Accuracy\n",
    "2. AUC: Area Under Curve\n",
    "3. Precision\n",
    "4. Recall\n",
    "5. F1\n",
    "6. Matthews Correlation Coefficient\n",
    "7. Lift\n",
    "\n",
    "Furthermore, we can evaluate the classification performance using a confusion matrix.\n",
    "\n",
    "\n",
    "Also plotting makes the evaluation of a logistic regression model a lot easier. We will first use a **Receiver Operating Characteristic (ROC) curve**, which is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis). Next, the **Cumulative Gains curve**, **Cumulative Lift curve** and **Cumulative Response curve** can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.evaluation import ClassificationEvaluator\n",
    "\n",
    "# get numpy array of True target labels and predicted scores:\n",
    "y_true = basetable[basetable[\"split\"] == \"selection\"][target_col].values\n",
    "y_pred = model.score_model(basetable[basetable[\"split\"] == \"selection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClassificationEvaluator()\n",
    "evaluator.fit(y_true, y_pred)  # Automatically find the best cut-off probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.scalar_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_cumulative_gains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_lift_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_cumulative_response_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can also compute the output needed to plot the so-called Predictor Insights Graphs (PIGs in short). These are graphs that represents the insights of the relationship between a single predictor and the target. This is a graph where the predictor is binned into groups, and where we represent group size in bars and group (target) incidence in a colored line. We have the option to force order of predictor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.evaluation import generate_pig_tables\n",
    "predictor_list = [col for col in basetable.columns\n",
    "                  if col.endswith(\"_bin\") or col.endswith(\"_processed\")]\n",
    "pig_tables = generate_pig_tables(basetable[basetable[\"split\"] == \"selection\"],\n",
    "                                 id_column_name=id_col,\n",
    "                                 target_column_name=target_col,\n",
    "                                 preprocessed_predictors=predictor_list)\n",
    "pig_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.evaluation import plot_incidence\n",
    "for predictor in list(pig_tables.variable.unique()):\n",
    "    print(predictor)\n",
    "    try:\n",
    "        if predictor + \"_bin\" in basetable.columns:\n",
    "            column_order = list(basetable[predictor + \"_bin\"].unique().sort_values())\n",
    "        else:\n",
    "            column_order = None #sorted(list(basetable[predictor].unique())) # e.g. just binary variable\n",
    "        plot_incidence(pig_tables,\n",
    "                       variable=predictor,\n",
    "                       model_type=\"classification\",\n",
    "                       column_order=column_order)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Can't plot PIG for {predictor}. Error was: {ve}\")\n",
    "    except TypeError as ve:\n",
    "        print(f\"Can't plot PIG for {predictor}. Error was: {ve}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
